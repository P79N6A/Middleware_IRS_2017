<?xml version="1.0" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>XL - Xen management tool, based on LibXenlight</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rev="made" href="mailto:root@localhost" />
</head>

<body style="background-color: white">


<!-- INDEX BEGIN -->
<div name="index">
<p><a name="__index__"></a></p>

<ul>

	<li><a href="#name">NAME</a></li>
	<li><a href="#synopsis">SYNOPSIS</a></li>
	<li><a href="#description">DESCRIPTION</a></li>
	<li><a href="#notes">NOTES</a></li>
	<li><a href="#global_options">GLOBAL OPTIONS</a></li>
	<li><a href="#domain_subcommands">DOMAIN SUBCOMMANDS</a></li>
	<li><a href="#xen_host_subcommands">XEN HOST SUBCOMMANDS</a></li>
	<li><a href="#scheduler_subcommands">SCHEDULER SUBCOMMANDS</a></li>
	<li><a href="#cpupools_commands">CPUPOOLS COMMANDS</a></li>
	<li><a href="#virtual_device_commands">VIRTUAL DEVICE COMMANDS</a></li>
	<ul>

		<li><a href="#block_devices">BLOCK DEVICES</a></li>
		<li><a href="#network_devices">NETWORK DEVICES</a></li>
		<li><a href="#channel_devices">CHANNEL DEVICES</a></li>
		<li><a href="#vtpm_devices">VTPM DEVICES</a></li>
	</ul>

	<li><a href="#pci_pass_through">PCI PASS-THROUGH</a></li>
	<li><a href="#tmem">TMEM</a></li>
	<li><a href="#flask">FLASK</a></li>
	<li><a href="#cache_monitoring_technology">CACHE MONITORING TECHNOLOGY</a></li>
	<li><a href="#to_be_documented">TO BE DOCUMENTED</a></li>
	<li><a href="#see_also">SEE ALSO</a></li>
	<li><a href="#bugs">BUGS</a></li>
</ul>

<hr name="index" />
</div>
<!-- INDEX END -->

<p>
</p>
<h1><a name="name">NAME</a></h1>
<p>XL - Xen management tool, based on LibXenlight</p>
<p>
</p>
<hr />
<h1><a name="synopsis">SYNOPSIS</a></h1>
<p><strong>xl</strong> <em>subcommand</em> [<em>args</em>]</p>
<p>
</p>
<hr />
<h1><a name="description">DESCRIPTION</a></h1>
<p>The <strong>xl</strong> program is the new tool for managing Xen guest
domains. The program can be used to create, pause, and shutdown
domains. It can also be used to list current domains, enable or pin
VCPUs, and attach or detach virtual block devices.</p>
<p>The basic structure of every <strong>xl</strong> command is almost always:</p>
<p><strong>xl</strong> <em>subcommand</em> [<em>OPTIONS</em>] <em>domain-id</em></p>
<p>Where <em>subcommand</em> is one of the subcommands listed below, <em>domain-id</em>
is the numeric domain id, or the domain name (which will be internally
translated to domain id), and <em>OPTIONS</em> are subcommand specific
options.  There are a few exceptions to this rule in the cases where
the subcommand in question acts on all domains, the entire machine,
or directly on the Xen hypervisor.  Those exceptions will be clear for
each of those subcommands.</p>
<p>
</p>
<hr />
<h1><a name="notes">NOTES</a></h1>
<dl>
<dt><strong><a name="start_the_script_etc_init_d_xencommons_at_boot_time" class="item">start the script <strong>/etc/init.d/xencommons</strong> at boot time</a></strong></dt>

<dd>
<p>Most <strong>xl</strong> operations rely upon <strong>xenstored</strong> and <strong>xenconsoled</strong>: make
sure you start the script <strong>/etc/init.d/xencommons</strong> at boot time to
initialize all the daemons needed by <strong>xl</strong>.</p>
</dd>
<dt><strong><a name="setup_a_xenbr0_bridge_in_dom0" class="item">setup a <strong>xenbr0</strong> bridge in dom0</a></strong></dt>

<dd>
<p>In the most common network configuration, you need to setup a bridge in dom0
named <strong>xenbr0</strong> in order to have a working network in the guest domains.
Please refer to the documentation of your Linux distribution to know how to
setup the bridge.</p>
</dd>
<dt><strong><a name="autoballoon" class="item"><strong>autoballoon</strong></a></strong></dt>

<dd>
<p>If you specify the amount of memory dom0 has, passing <strong>dom0_mem</strong> to
Xen, it is highly recommended to disable <strong>autoballoon</strong>. Edit
<strong>/etc/xen/xl.conf</strong> and set it to 0.</p>
</dd>
<dt><strong><a name="run_xl_as_root" class="item">run xl as <strong>root</strong></a></strong></dt>

<dd>
<p>Most <strong>xl</strong> commands require root privileges to run due to the
communications channels used to talk to the hypervisor.  Running as
non root will return an error.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="global_options">GLOBAL OPTIONS</a></h1>
<p>Some global options are always available:</p>
<dl>
<dt><strong><a name="v" class="item"><strong>-v</strong></a></strong></dt>

<dd>
<p>Verbose.</p>
</dd>
<dt><strong><a name="n" class="item"><strong>-N</strong></a></strong></dt>

<dd>
<p>Dry run: do not actually execute the command.</p>
</dd>
<dt><strong><a name="f" class="item"><strong>-f</strong></a></strong></dt>

<dd>
<p>Force execution: xl will refuse to run some commands if it detects that xend is
also running, this option will force the execution of those commands, even
though it is unsafe.</p>
</dd>
<dt><strong><a name="t" class="item"><strong>-t</strong></a></strong></dt>

<dd>
<p>Always use carriage-return-based overwriting for printing progress
messages without scrolling the screen.  Without -t, this is done only
if stderr is a tty.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="domain_subcommands">DOMAIN SUBCOMMANDS</a></h1>
<p>The following subcommands manipulate domains directly.  As stated
previously, most commands take <em>domain-id</em> as the first parameter.</p>
<dl>
<dt><strong><a name="button_press_domain_id_button" class="item"><strong>button-press</strong> <em>domain-id</em> <em>button</em></a></strong></dt>

<dd>
<p><em>This command is deprecated. Please use <code>xl trigger</code> in preference</em></p>
<p>Indicate an ACPI button press to the domain. <em>button</em> is may be 'power' or
'sleep'. This command is only available for HVM domains.</p>
</dd>
<dt><strong><a name="create_configfile_options" class="item"><strong>create</strong> [<em>configfile</em>] [<em>OPTIONS</em>]</a></strong></dt>

<dd>
<p>The create subcommand takes a config file as first argument: see
<em>xl.cfg</em> for full details of that file format and possible options.
If <em>configfile</em> is missing <strong>XL</strong> creates the domain starting from the
default value for every option.</p>
<p><em>configfile</em> has to be an absolute path to a file.</p>
<p>Create will return <strong>as soon</strong> as the domain is started.  This <strong>does
not</strong> mean the guest OS in the domain has actually booted, or is
available for input.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="q_quiet" class="item"><strong>-q</strong>, <strong>--quiet</strong></a></strong></dt>

<dd>
<p>No console output.</p>
</dd>
<dt><strong><a name="f_file_defconfig_file" class="item"><strong>-f=FILE</strong>, <strong>--defconfig=FILE</strong></a></strong></dt>

<dd>
<p>Use the given configuration file.</p>
</dd>
<dt><strong><a name="p" class="item"><strong>-p</strong></a></strong></dt>

<dd>
<p>Leave the domain paused after it is created.</p>
</dd>
<dt><strong><a name="v_vncviewer" class="item"><strong>-V</strong>, <strong>--vncviewer</strong></a></strong></dt>

<dd>
<p>Attach to domain's VNC server, forking a vncviewer process.</p>
</dd>
<dt><strong><a name="a_vncviewer_autopass" class="item"><strong>-A</strong>, <strong>--vncviewer-autopass</strong></a></strong></dt>

<dd>
<p>Pass VNC password to vncviewer via stdin.</p>
</dd>
<dt><strong><a name="c" class="item"><strong>-c</strong></a></strong></dt>

<dd>
<p>Attach console to the domain as soon as it has started.  This is
useful for determining issues with crashing domains and just as a
general convenience since you often want to watch the
domain boot.</p>
</dd>
<dt><strong><a name="key_value" class="item"><strong>key=value</strong></a></strong></dt>

<dd>
<p>It is possible to pass <em>key=value</em> pairs on the command line to provide
options as if they were written in the configuration file; these override
whatever is in the <em>configfile</em>.</p>
<p>NB: Many config options require characters such as quotes or brackets
which are interpreted by the shell (and often discarded) before being
passed to xl, resulting in xl being unable to parse the value
correctly.  A simple work-around is to put all extra options within a
single set of quotes, separated by semicolons.  (See below for an example.)</p>
</dd>
</dl>
<p><strong>EXAMPLES</strong></p>
<dl>
<dt><strong><a name="with_config_file" class="item"><em>with config file</em></a></strong></dt>

<dd>
<pre>
  xl create DebianLenny</pre>
<p>This creates a domain with the file /etc/xen/DebianLenny, and returns as
soon as it is run.</p>
</dd>
<dt><strong><a name="with_extra_parameters" class="item"><em>with extra parameters</em></a></strong></dt>

<dd>
<pre>
  xl create hvm.cfg 'cpus=&quot;0-3&quot;; pci=[&quot;01:05.1&quot;,&quot;01:05.2&quot;]'</pre>
<p>This creates a domain with the file hvm.cfg, but additionally pins it to
cpus 0-3, and passes through two PCI devices.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="config_update_domid_configfile_options" class="item"><strong>config-update</strong> <strong>domid</strong> [<em>configfile</em>] [<em>OPTIONS</em>]</a></strong></dt>

<dd>
<p>Update the saved configuration for a running domain. This has no
immediate effect but will be applied when the guest is next
restarted. This command is useful to ensure that runtime modifications
made to the guest will be preserved when the guest is restarted.</p>
<p>Since Xen 4.5 xl has improved capabilities to handle dynamic domain
configuration changes and will preserve any changes made a runtime
when necessary. Therefore it should not normally be necessary to use
this command any more.</p>
<p><em>configfile</em> has to be an absolute path to a file.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="f_file_defconfig_file2" class="item"><strong>-f=FILE</strong>, <strong>--defconfig=FILE</strong></a></strong></dt>

<dd>
<p>Use the given configuration file.</p>
</dd>
<dt><strong><a name="key_value2" class="item"><strong>key=value</strong></a></strong></dt>

<dd>
<p>It is possible to pass <em>key=value</em> pairs on the command line to
provide options as if they were written in the configuration file;
these override whatever is in the <em>configfile</em>.  Please see the note
under <em>create</em> on handling special characters when passing
<em>key=value</em> pairs on the command line.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="console_options_domain_id" class="item"><strong>console</strong> [<em>OPTIONS</em>] <em>domain-id</em></a></strong></dt>

<dd>
<p>Attach to domain <em>domain-id</em>'s console.  If you've set up your domains to
have a traditional log in console this will look much like a normal
text log in screen.</p>
<p>Use the key combination Ctrl+] to detach the domain console.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="t_pv_serial" class="item"><em>-t [pv|serial]</em></a></strong></dt>

<dd>
<p>Connect to a PV console or connect to an emulated serial console.
PV consoles are the only consoles available for PV domains while HVM
domains can have both. If this option is not specified it defaults to
emulated serial for HVM guests and PV console for PV guests.</p>
</dd>
<dt><strong><a name="n_num" class="item"><em>-n NUM</em></a></strong></dt>

<dd>
<p>Connect to console number <em>NUM</em>. Console numbers start from 0.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="destroy_options_domain_id" class="item"><strong>destroy</strong> [<em>OPTIONS</em>] <em>domain-id</em></a></strong></dt>

<dd>
<p>Immediately terminate the domain <em>domain-id</em>.  This doesn't give the
domain OS any chance to react, and is the equivalent of ripping the
power cord out on a physical machine.  In most cases you will want to
use the <strong>shutdown</strong> command instead.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="f2" class="item"><em>-f</em></a></strong></dt>

<dd>
<p>Allow domain 0 to be destroyed.  Because domain cannot destroy itself, this is
only possible when using a disaggregated toolstack, and is most useful when
using a hardware domain separated from domain 0.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="domid_domain_name" class="item"><strong>domid</strong> <em>domain-name</em></a></strong></dt>

<dd>
<p>Converts a domain name to a domain id.</p>
</dd>
<dt><strong><a name="domname_domain_id" class="item"><strong>domname</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>Converts a domain id to a domain name.</p>
</dd>
<dt><strong><a name="rename_domain_id_new_name" class="item"><strong>rename</strong> <em>domain-id</em> <em>new-name</em></a></strong></dt>

<dd>
<p>Change the domain name of <em>domain-id</em> to <em>new-name</em>.</p>
</dd>
<dt><strong><a name="dump_core_domain_id_filename" class="item"><strong>dump-core</strong> <em>domain-id</em> [<em>filename</em>]</a></strong></dt>

<dd>
<p>Dumps the virtual machine's memory for the specified domain to the
<em>filename</em> specified, without pausing the domain.  The dump file will
be written to a distribution specific directory for dump files.  Such
as: /var/lib/xen/dump or /var/xen/dump.</p>
</dd>
<dt><strong><a name="help_long" class="item"><strong>help</strong> [<em>--long</em>]</a></strong></dt>

<dd>
<p>Displays the short help message (i.e. common commands).</p>
<p>The <em>--long</em> option prints out the complete set of <strong>xl</strong> subcommands,
grouped by function.</p>
</dd>
<dt><strong><a name="list_options_domain_id" class="item"><strong>list</strong> [<em>OPTIONS</em>] [<em>domain-id</em> ...]</a></strong></dt>

<dd>
<p>Prints information about one or more domains.  If no domains are
specified it prints out information about all domains.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="l_long" class="item"><strong>-l</strong>, <strong>--long</strong></a></strong></dt>

<dd>
<p>The output for <strong>xl list</strong> is not the table view shown below, but 
instead presents the data in as a JSON data structure.</p>
</dd>
<dt><strong><a name="z_context_also_prints_the_security_labels" class="item"><strong>-Z</strong>, <strong>--context</strong>
Also prints the security labels.</a></strong></dt>

<dt><strong><a name="v_verbose" class="item"><strong>-v</strong>, <strong>--verbose</strong></a></strong></dt>

<dd>
<p>Also prints the domain UUIDs, the shutdown reason and security labels.</p>
</dd>
</dl>
<p><strong>EXAMPLE</strong></p>
<p>An example format for the list is as follows:</p>
<pre>
    Name                                        ID   Mem VCPUs      State   Time(s)
    Domain-0                                     0   750     4     r-----   11794.3
    win                                          1  1019     1     r-----       0.3
    linux                                        2  2048     2     r-----    5624.2</pre>
<p>Name is the name of the domain.  ID the numeric domain id.  Mem is the
desired amount of memory to allocate to the domain (although it may
not be the currently allocated amount).  VCPUs is the number of
virtual CPUs allocated to the domain.  State is the run state (see
below).  Time is the total run time of the domain as accounted for by
Xen.</p>
<p><strong>STATES</strong></p>
<p>The State field lists 6 states for a Xen domain, and which ones the
current domain is in.</p>
<dl>
<dt><strong><a name="r_running" class="item"><strong>r - running</strong></a></strong></dt>

<dd>
<p>The domain is currently running on a CPU.</p>
</dd>
<dt><strong><a name="b_blocked" class="item"><strong>b - blocked</strong></a></strong></dt>

<dd>
<p>The domain is blocked, and not running or runnable.  This can be caused
because the domain is waiting on IO (a traditional wait state) or has
gone to sleep because there was nothing else for it to do.</p>
</dd>
<dt><strong><a name="p_paused" class="item"><strong>p - paused</strong></a></strong></dt>

<dd>
<p>The domain has been paused, usually occurring through the administrator
running <strong>xl pause</strong>.  When in a paused state the domain will still
consume allocated resources like memory, but will not be eligible for
scheduling by the Xen hypervisor.</p>
</dd>
<dt><strong><a name="s_shutdown" class="item"><strong>s - shutdown</strong></a></strong></dt>

<dd>
<p>The guest OS has shut down (SCHEDOP_shutdown has been called) but the
domain is not dying yet.</p>
</dd>
<dt><strong><a name="c_crashed" class="item"><strong>c - crashed</strong></a></strong></dt>

<dd>
<p>The domain has crashed, which is always a violent ending.  Usually
this state can only occur if the domain has been configured not to
restart on crash.  See <em>xl.cfg(5)</em> for more info.</p>
</dd>
<dt><strong><a name="d_dying" class="item"><strong>d - dying</strong></a></strong></dt>

<dd>
<p>The domain is in process of dying, but hasn't completely shutdown or
crashed.</p>
</dd>
</dl>
<p><strong>NOTES</strong></p>
<p>The Time column is deceptive.  Virtual IO (network and block devices)
used by domains requires coordination by Domain0, which means that
Domain0 is actually charged for much of the time that a DomainU is
doing IO.  Use of this time value to determine relative utilizations
by domains is thus very suspect, as a high IO workload may show as
less utilized than a high CPU workload.  Consider yourself warned.</p>
</dd>
<dt><strong><a name="mem_max_domain_id_mem" class="item"><strong>mem-max</strong> <em>domain-id</em> <em>mem</em></a></strong></dt>

<dd>
<p>Specify the maximum amount of memory the domain is able to use, appending 't'
for terabytes, 'g' for gigabytes, 'm' for megabytes, 'k' for kilobytes and 'b'
for bytes.</p>
<p>The mem-max value may not correspond to the actual memory used in the
domain, as it may balloon down its memory to give more back to the OS.</p>
</dd>
<dt><strong><a name="mem_set_domain_id_mem" class="item"><strong>mem-set</strong> <em>domain-id</em> <em>mem</em></a></strong></dt>

<dd>
<p>Set the domain's used memory using the balloon driver; append 't' for
terabytes, 'g' for gigabytes, 'm' for megabytes, 'k' for kilobytes and 'b' for
bytes.</p>
<p>Because this operation requires cooperation from the domain operating
system, there is no guarantee that it will succeed.  This command will
definitely not work unless the domain has the required paravirt
driver.</p>
<p><strong>Warning:</strong> There is no good way to know in advance how small of a
mem-set will make a domain unstable and cause it to crash.  Be very
careful when using this command on running domains.</p>
</dd>
<dt><strong><a name="migrate_options_domain_id_host" class="item"><strong>migrate</strong> [<em>OPTIONS</em>] <em>domain-id</em> <em>host</em></a></strong></dt>

<dd>
<p>Migrate a domain to another host machine. By default <strong>xl</strong> relies on ssh as a
transport mechanism between the two hosts.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="s_sshcommand" class="item"><strong>-s</strong> <em>sshcommand</em></a></strong></dt>

<dd>
<p>Use &lt;sshcommand&gt; instead of ssh.  String will be passed to sh. If empty, run
&lt;host&gt; instead of ssh &lt;host&gt; xl migrate-receive [-d -e].</p>
</dd>
<dt><strong><a name="e" class="item"><strong>-e</strong></a></strong></dt>

<dd>
<p>On the new host, do not wait in the background (on &lt;host&gt;) for the death of the
domain. See the corresponding option of the <em>create</em> subcommand.</p>
</dd>
<dt><strong><a name="c_config" class="item"><strong>-C</strong> <em>config</em></a></strong></dt>

<dd>
<p>Send &lt;config&gt; instead of config file from creation.</p>
</dd>
<dt><strong><a name="debug" class="item"><strong>--debug</strong></a></strong></dt>

<dd>
<p>Print huge (!) amount of debug during the migration process.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="remus_options_domain_id_host" class="item"><strong>remus</strong> [<em>OPTIONS</em>] <em>domain-id</em> <em>host</em></a></strong></dt>

<dd>
<p>Enable Remus HA for domain. By default <strong>xl</strong> relies on ssh as a transport
mechanism between the two hosts.</p>
<p>N.B: Remus support in xl is still in experimental (proof-of-concept) phase.
     Disk replication support is limited to DRBD disks.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="i_ms" class="item"><strong>-i</strong> <em>MS</em></a></strong></dt>

<dd>
<p>Checkpoint domain memory every MS milliseconds (default 200ms).</p>
</dd>
<dt><strong><a name="u" class="item"><strong>-u</strong></a></strong></dt>

<dd>
<p>Disable memory checkpoint compression.</p>
</dd>
<dt><strong><a name="s_sshcommand2" class="item"><strong>-s</strong> <em>sshcommand</em></a></strong></dt>

<dd>
<p>Use &lt;sshcommand&gt; instead of ssh.  String will be passed to sh.
If empty, run &lt;host&gt; instead of ssh &lt;host&gt; xl migrate-receive -r [-e].</p>
</dd>
<dt><strong><a name="e2" class="item"><strong>-e</strong></a></strong></dt>

<dd>
<p>On the new host, do not wait in the background (on &lt;host&gt;) for the death
of the domain. See the corresponding option of the <em>create</em> subcommand.</p>
</dd>
<dt><strong><a name="n_netbufscript" class="item"><strong>-N</strong> <em>netbufscript</em></a></strong></dt>

<dd>
<p>Use &lt;netbufscript&gt; to setup network buffering instead of the
default script (/etc/xen/scripts/remus-netbuf-setup).</p>
</dd>
<dt><strong><a name="f" class="item"><strong>-F</strong></a></strong></dt>

<dd>
<p>Run Remus in unsafe mode. Use this option with caution as failover may
not work as intended.</p>
</dd>
<dt><strong><a name="b" class="item"><strong>-b</strong></a></strong></dt>

<dd>
<p>Replicate memory checkpoints to /dev/null (blackhole).
Generally useful for debugging. Requires enabling unsafe mode.</p>
</dd>
<dt><strong><a name="n" class="item"><strong>-n</strong></a></strong></dt>

<dd>
<p>Disable network output buffering. Requires enabling unsafe mode.</p>
</dd>
<dt><strong><a name="d" class="item"><strong>-d</strong></a></strong></dt>

<dd>
<p>Disable disk replication. Requires enabling unsafe mode.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="pause_domain_id" class="item"><strong>pause</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>Pause a domain.  When in a paused state the domain will still consume
allocated resources such as memory, but will not be eligible for
scheduling by the Xen hypervisor.</p>
</dd>
<dt><strong><a name="reboot_options_domain_id" class="item"><strong>reboot</strong> [<em>OPTIONS</em>] <em>domain-id</em></a></strong></dt>

<dd>
<p>Reboot a domain.  This acts just as if the domain had the <strong>reboot</strong>
command run from the console.  The command returns as soon as it has
executed the reboot action, which may be significantly before the
domain actually reboots.</p>
<p>For HVM domains this requires PV drivers to be installed in your guest
OS. If PV drivers are not present but you have configured the guest OS
to behave appropriately you may be able to use the <em>-F</em> option
trigger a reset button press.</p>
<p>The behavior of what happens to a domain when it reboots is set by the
<strong>on_reboot</strong> parameter of the domain configuration file when the
domain was created.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="f2" class="item"><strong>-F</strong></a></strong></dt>

<dd>
<p>If the guest does not support PV reboot control then fallback to
sending an ACPI power event (equivalent to the <em>reset</em> option to
<em>trigger</em>.</p>
<p>You should ensure that the guest is configured to behave as expected
in response to this event.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="restore_options_configfile_checkpointfile" class="item"><strong>restore</strong> [<em>OPTIONS</em>] [<em>ConfigFile</em>] <em>CheckpointFile</em></a></strong></dt>

<dd>
<p>Build a domain from an <strong>xl save</strong> state file.  See <strong>save</strong> for more info.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="p2" class="item"><strong>-p</strong></a></strong></dt>

<dd>
<p>Do not unpause domain after restoring it.</p>
</dd>
<dt><strong><a name="e3" class="item"><strong>-e</strong></a></strong></dt>

<dd>
<p>Do not wait in the background for the death of the domain on the new host.
See the corresponding option of the <em>create</em> subcommand.</p>
</dd>
<dt><strong><a name="d2" class="item"><strong>-d</strong></a></strong></dt>

<dd>
<p>Enable debug messages.</p>
</dd>
<dt><strong><a name="v_vncviewer2" class="item"><strong>-V</strong>, <strong>--vncviewer</strong></a></strong></dt>

<dd>
<p>Attach to domain's VNC server, forking a vncviewer process.</p>
</dd>
<dt><strong><a name="a_vncviewer_autopass2" class="item"><strong>-A</strong>, <strong>--vncviewer-autopass</strong></a></strong></dt>

<dd>
<p>Pass VNC password to vncviewer via stdin.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="save_options_domain_id_checkpointfile_configfile" class="item"><strong>save</strong> [<em>OPTIONS</em>] <em>domain-id</em> <em>CheckpointFile</em> [<em>ConfigFile</em>]</a></strong></dt>

<dd>
<p>Saves a running domain to a state file so that it can be restored
later.  Once saved, the domain will no longer be running on the
system, unless the -c or -p options are used.
<strong>xl restore</strong> restores from this checkpoint file.
Passing a config file argument allows the user to manually select the VM config
file used to create the domain.</p>
<dl>
<dt><strong><a name="c2" class="item"><strong>-c</strong></a></strong></dt>

<dd>
<p>Leave domain running after creating the snapshot.</p>
</dd>
<dt><strong><a name="p3" class="item"><strong>-p</strong></a></strong></dt>

<dd>
<p>Leave domain paused after creating the snapshot.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="sharing_domain_id" class="item"><strong>sharing</strong> [<em>domain-id</em>]</a></strong></dt>

<dd>
<p>List count of shared pages.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="domain_id" class="item"><em>domain_id</em></a></strong></dt>

<dd>
<p>List specifically for that domain. Otherwise, list for all domains.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="shutdown_options_a_domain_id" class="item"><strong>shutdown</strong> [<em>OPTIONS</em>] <em>-a|domain-id</em></a></strong></dt>

<dd>
<p>Gracefully shuts down a domain.  This coordinates with the domain OS
to perform graceful shutdown, so there is no guarantee that it will
succeed, and may take a variable length of time depending on what
services must be shutdown in the domain.</p>
<p>For HVM domains this requires PV drivers to be installed in your guest
OS. If PV drivers are not present but you have configured the guest OS
to behave appropriately you may be able to use the <em>-F</em> option
trigger a power button press.</p>
<p>The command returns immediately after signally the domain unless that
<strong>-w</strong> flag is used.</p>
<p>The behavior of what happens to a domain when it reboots is set by the
<strong>on_shutdown</strong> parameter of the domain configuration file when the
domain was created.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="a_all" class="item"><strong>-a</strong>, <strong>--all</strong></a></strong></dt>

<dd>
<p>Shutdown all guest domains.  Often used when doing a complete shutdown
of a Xen system.</p>
</dd>
<dt><strong><a name="w_wait" class="item"><strong>-w</strong>, <strong>--wait</strong></a></strong></dt>

<dd>
<p>Wait for the domain to complete shutdown before returning.</p>
</dd>
<dt><strong><a name="f3" class="item"><strong>-F</strong></a></strong></dt>

<dd>
<p>If the guest does not support PV shutdown control then fallback to
sending an ACPI power event (equivalent to the <em>power</em> option to
<em>trigger</em>.</p>
<p>You should ensure that the guest is configured to behave as expected
in response to this event.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="sysrq_domain_id_letter" class="item"><strong>sysrq</strong> <em>domain-id</em> <em>letter</em></a></strong></dt>

<dd>
<p>Send a &lt;Magic System Request&gt; to the domain, each type of request is
represented by a different letter.
It can be used to send SysRq requests to Linux guests, see sysrq.txt in
your Linux Kernel sources for more information.
It requires PV drivers to be installed in your guest OS.</p>
</dd>
<dt><strong><a name="trigger_domain_id_nmi_reset_init_power_sleep_s3resume_vcpu" class="item"><strong>trigger</strong> <em>domain-id</em> <em>nmi|reset|init|power|sleep|s3resume</em> [<em>VCPU</em>]</a></strong></dt>

<dd>
<p>Send a trigger to a domain, where the trigger can be: nmi, reset, init, power
or sleep.  Optionally a specific vcpu number can be passed as an argument.
This command is only available for HVM domains.</p>
</dd>
<dt><strong><a name="unpause_domain_id" class="item"><strong>unpause</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>Moves a domain out of the paused state.  This will allow a previously
paused domain to now be eligible for scheduling by the Xen hypervisor.</p>
</dd>
<dt><strong><a name="vcpu_set_domain_id_vcpu_count" class="item"><strong>vcpu-set</strong> <em>domain-id</em> <em>vcpu-count</em></a></strong></dt>

<dd>
<p>Enables the <em>vcpu-count</em> virtual CPUs for the domain in question.
Like mem-set, this command can only allocate up to the maximum virtual
CPU count configured at boot for the domain.</p>
<p>If the <em>vcpu-count</em> is smaller than the current number of active
VCPUs, the highest number VCPUs will be hotplug removed.  This may be
important for pinning purposes.</p>
<p>Attempting to set the VCPUs to a number larger than the initially
configured VCPU count is an error.  Trying to set VCPUs to &lt; 1 will be
quietly ignored.</p>
<p>Some guests may need to actually bring the newly added CPU online
after <strong>vcpu-set</strong>, go to <strong>SEE ALSO</strong> section for information.</p>
</dd>
<dt><strong><a name="vcpu_list_domain_id" class="item"><strong>vcpu-list</strong> [<em>domain-id</em>]</a></strong></dt>

<dd>
<p>Lists VCPU information for a specific domain.  If no domain is
specified, VCPU information for all domains will be provided.</p>
</dd>
<dt><strong><a name="vcpu_pin_domain_id_vcpu_cpus_hard_cpus_soft" class="item"><strong>vcpu-pin</strong> <em>domain-id</em> <em>vcpu</em> <em>cpus hard</em> <em>cpus soft</em></a></strong></dt>

<dd>
<p>Set hard and soft affinity for a <em>vcpu</em> of &lt;domain-id&gt;. Normally VCPUs
can float between available CPUs whenever Xen deems a different run state
is appropriate.</p>
<p>Hard affinity can be used to restrict this, by ensuring certain VCPUs
can only run on certain physical CPUs. Soft affinity specifies a <em>preferred</em>
set of CPUs. Soft affinity needs special support in the scheduler, which is
only provided in credit1.</p>
<p>The keyword <strong>all</strong> can be used to apply the hard and soft affinity masks to
all the VCPUs in the domain. The symbol '-' can be used to leave either
hard or soft affinity alone.</p>
<p>For example:</p>
<pre>
 xl vcpu-pin 0 3 - 6-9</pre>
<p>will set soft affinity for vCPU 3 of domain 0 to pCPUs 6,7,8 and 9,
leaving its hard affinity untouched. On the othe hand:</p>
<pre>
 xl vcpu-pin 0 3 3,4 6-9</pre>
<p>will set both hard and soft affinity, the former to pCPUs 3 and 4, the
latter to pCPUs 6,7,8, and 9.</p>
</dd>
<dt><strong><a name="vm_list" class="item"><strong>vm-list</strong></a></strong></dt>

<dd>
<p>Prints information about guests. This list excludes information about
service or auxiliary domains such as dom0 and stubdoms.</p>
<p><strong>EXAMPLE</strong></p>
<p>An example format for the list is as follows:</p>
<pre>
    UUID                                  ID    name
    59e1cf6c-6ab9-4879-90e7-adc8d1c63bf5  2    win
    50bc8f75-81d0-4d53-b2e6-95cb44e2682e  3    linux</pre>
</dd>
<dt><strong><a name="vncviewer_options_domain_id" class="item"><strong>vncviewer</strong> [<em>OPTIONS</em>] <em>domain-id</em></a></strong></dt>

<dd>
<p>Attach to domain's VNC server, forking a vncviewer process.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="autopass" class="item"><em>--autopass</em></a></strong></dt>

<dd>
<p>Pass VNC password to vncviewer via stdin.</p>
</dd>
</dl>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="xen_host_subcommands">XEN HOST SUBCOMMANDS</a></h1>
<dl>
<dt><strong><a name="debug_keys_keys" class="item"><strong>debug-keys</strong> <em>keys</em></a></strong></dt>

<dd>
<p>Send debug <em>keys</em> to Xen. It is the same as pressing the Xen
&quot;conswitch&quot; (Ctrl-A by default) three times and then pressing &quot;keys&quot;.</p>
</dd>
<dt><strong><a name="dmesg_c" class="item"><strong>dmesg</strong> [<strong>-c</strong>]</a></strong></dt>

<dd>
<p>Reads the Xen message buffer, similar to dmesg on a Linux system.  The
buffer contains informational, warning, and error messages created
during Xen's boot process.  If you are having problems with Xen, this
is one of the first places to look as part of problem determination.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="c_clear" class="item"><strong>-c</strong>, <strong>--clear</strong></a></strong></dt>

<dd>
<p>Clears Xen's message buffer.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="info_n_numa" class="item"><strong>info</strong> [<strong>-n</strong>, <strong>--numa</strong>]</a></strong></dt>

<dd>
<p>Print information about the Xen host in <em>name : value</em> format.  When
reporting a Xen bug, please provide this information as part of the
bug report. See <em><a href="http://wiki.xen.org/xenwiki/ReportingBugs">http://wiki.xen.org/xenwiki/ReportingBugs</a></em> on how to
report Xen bugs.</p>
<p>Sample output looks as follows:</p>
<pre>
 host                   : scarlett
 release                : 3.1.0-rc4+
 version                : #1001 SMP Wed Oct 19 11:09:54 UTC 2011
 machine                : x86_64
 nr_cpus                : 4
 nr_nodes               : 1
 cores_per_socket       : 4
 threads_per_core       : 1
 cpu_mhz                : 2266
 hw_caps                : bfebfbff:28100800:00000000:00003b40:009ce3bd:00000000:00000001:00000000
 virt_caps              : hvm hvm_directio
 total_memory           : 6141
 free_memory            : 4274
 free_cpus              : 0
 outstanding_claims     : 0
 xen_major              : 4
 xen_minor              : 2
 xen_extra              : -unstable
 xen_caps               : xen-3.0-x86_64 xen-3.0-x86_32p hvm-3.0-x86_32 hvm-3.0-x86_32p hvm-3.0-x86_64 
 xen_scheduler          : credit
 xen_pagesize           : 4096
 platform_params        : virt_start=0xffff800000000000
 xen_changeset          : Wed Nov 02 17:09:09 2011 +0000 24066:54a5e994a241
 xen_commandline        : com1=115200,8n1 guest_loglvl=all dom0_mem=750M console=com1 
 cc_compiler            : gcc version 4.4.5 (Debian 4.4.5-8) 
 cc_compile_by          : sstabellini
 cc_compile_domain      : uk.xensource.com
 cc_compile_date        : Tue Nov  8 12:03:05 UTC 2011
 xend_config_format     : 4</pre>
<p><strong>FIELDS</strong></p>
<p>Not all fields will be explained here, but some of the less obvious
ones deserve explanation:</p>
<dl>
<dt><strong><a name="hw_caps" class="item"><strong>hw_caps</strong></a></strong></dt>

<dd>
<p>A vector showing what hardware capabilities are supported by your
processor.  This is equivalent to, though more cryptic, the flags
field in /proc/cpuinfo on a normal Linux machine: they both derive from
the feature bits returned by the cpuid command on x86 platforms.</p>
</dd>
<dt><strong><a name="free_memory" class="item"><strong>free_memory</strong></a></strong></dt>

<dd>
<p>Available memory (in MB) not allocated to Xen, or any other domains, or
claimed for domains.</p>
</dd>
<dt><strong><a name="outstanding_claims" class="item"><strong>outstanding_claims</strong></a></strong></dt>

<dd>
<p>When a claim call is done (see <em>xl.conf</em>) a reservation for a specific
amount of pages is set and also a global value is incremented. This
global value (outstanding_claims) is then reduced as the domain's memory
is populated and eventually reaches zero. Most of the time the value will
be zero, but if you are launching multiple guests, and <strong>claim_mode</strong> is
enabled, this value can increase/decrease. Note that the value also
affects the <strong>free_memory</strong>  - as it will reflect the free memory
in the hypervisor minus the outstanding pages claimed for guests.
See xl <em>info</em> <strong>claims</strong> parameter for detailed listing.</p>
</dd>
<dt><strong><a name="xen_caps" class="item"><strong>xen_caps</strong></a></strong></dt>

<dd>
<p>The Xen version and architecture.  Architecture values can be one of:
x86_32, x86_32p (i.e. PAE enabled), x86_64, ia64.</p>
</dd>
<dt><strong><a name="xen_changeset" class="item"><strong>xen_changeset</strong></a></strong></dt>

<dd>
<p>The Xen mercurial changeset id.  Very useful for determining exactly
what version of code your Xen system was built from.</p>
</dd>
</dl>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="n_numa" class="item"><strong>-n</strong>, <strong>--numa</strong></a></strong></dt>

<dd>
<p>List host NUMA topology information</p>
</dd>
</dl>
</dd>
<dt><strong><a name="top" class="item"><strong>top</strong></a></strong></dt>

<dd>
<p>Executes the <strong>xentop</strong> command, which provides real time monitoring of
domains.  Xentop is a curses interface, and reasonably self
explanatory.</p>
</dd>
<dt><strong><a name="uptime" class="item"><strong>uptime</strong></a></strong></dt>

<dd>
<p>Prints the current uptime of the domains running.</p>
</dd>
<dt><strong><a name="claims" class="item"><strong>claims</strong></a></strong></dt>

<dd>
<p>Prints information about outstanding claims by the guests. This provides
the outstanding claims and currently populated memory count for the guests.
These values added up reflect the global outstanding claim value, which
is provided via the <em>info</em> argument, <strong>outstanding_claims</strong> value.
The <strong>Mem</strong> column has the cumulative value of outstanding claims and
the total amount of memory that has been right now allocated to the guest.</p>
<p><strong>EXAMPLE</strong></p>
<p>An example format for the list is as follows:</p>
<pre>
 Name                                        ID   Mem VCPUs      State   Time(s)  Claimed
 Domain-0                                     0  2047     4     r-----      19.7     0
 OL5                                          2  2048     1     --p---       0.0   847
 OL6                                          3  1024     4     r-----       5.9     0
 Windows_XP                                   4  2047     1     --p---       0.0  1989</pre>
<p>In which it can be seen that the OL5 guest still has 847MB of claimed
memory (out of the total 2048MB where 1191MB has been allocated to
the guest).</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="scheduler_subcommands">SCHEDULER SUBCOMMANDS</a></h1>
<p>Xen ships with a number of domain schedulers, which can be set at boot
time with the <strong>sched=</strong> parameter on the Xen command line.  By
default <strong>credit</strong> is used for scheduling.</p>
<dl>
<dt><strong><a name="sched_credit_options" class="item"><strong>sched-credit</strong> [<em>OPTIONS</em>]</a></strong></dt>

<dd>
<p>Set or get credit scheduler parameters.  The credit scheduler is a
proportional fair share CPU scheduler built from the ground up to be
work conserving on SMP hosts.</p>
<p>Each domain (including Domain0) is assigned a weight and a cap.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="d_domain_domain_domain" class="item"><strong>-d DOMAIN</strong>, <strong>--domain=DOMAIN</strong></a></strong></dt>

<dd>
<p>Specify domain for which scheduler parameters are to be modified or retrieved.
Mandatory for modifying scheduler parameters.</p>
</dd>
<dt><strong><a name="w_weight_weight_weight" class="item"><strong>-w WEIGHT</strong>, <strong>--weight=WEIGHT</strong></a></strong></dt>

<dd>
<p>A domain with a weight of 512 will get twice as much CPU as a domain
with a weight of 256 on a contended host. Legal weights range from 1
to 65535 and the default is 256.</p>
</dd>
<dt><strong><a name="c_cap_cap_cap" class="item"><strong>-c CAP</strong>, <strong>--cap=CAP</strong></a></strong></dt>

<dd>
<p>The cap optionally fixes the maximum amount of CPU a domain will be
able to consume, even if the host system has idle CPU cycles. The cap
is expressed in percentage of one physical CPU: 100 is 1 physical CPU,
50 is half a CPU, 400 is 4 CPUs, etc. The default, 0, means there is
no upper cap.</p>
<p>NB: Many systems have features that will scale down the computing
power of a cpu that is not 100% utilized.  This can be in the
operating system, but can also sometimes be below the operating system
in the BIOS.  If you set a cap such that individual cores are running
at less than 100%, this may have an impact on the performance of your
workload over and above the impact of the cap. For example, if your
processor runs at 2GHz, and you cap a vm at 50%, the power management
system may also reduce the clock speed to 1GHz; the effect will be
that your VM gets 25% of the available power (50% of 1GHz) rather than
50% (50% of 2GHz).  If you are not getting the performance you expect,
look at performance and cpufreq options in your operating system and
your BIOS.</p>
</dd>
<dt><strong><a name="p_cpupool_cpupool_cpupool" class="item"><strong>-p CPUPOOL</strong>, <strong>--cpupool=CPUPOOL</strong></a></strong></dt>

<dd>
<p>Restrict output to domains in the specified cpupool.</p>
</dd>
<dt><strong><a name="s_schedparam" class="item"><strong>-s</strong>, <strong>--schedparam</strong></a></strong></dt>

<dd>
<p>Specify to list or set pool-wide scheduler parameters.</p>
</dd>
<dt><strong><a name="t_tslice_tslice_ms_tslice" class="item"><strong>-t TSLICE</strong>, <strong>--tslice_ms=TSLICE</strong></a></strong></dt>

<dd>
<p>Timeslice tells the scheduler how long to allow VMs to run before
pre-empting.  The default is 30ms.  Valid ranges are 1ms to 1000ms.
The length of the timeslice (in ms) must be higher than the length of
the ratelimit (see below).</p>
</dd>
<dt><strong><a name="r_rlimit_ratelimit_us_rlimit" class="item"><strong>-r RLIMIT</strong>, <strong>--ratelimit_us=RLIMIT</strong></a></strong></dt>

<dd>
<p>Ratelimit attempts to limit the number of schedules per second.  It
sets a minimum amount of time (in microseconds) a VM must run before
we will allow a higher-priority VM to pre-empt it.  The default value
is 1000 microseconds (1ms).  Valid range is 100 to 500000 (500ms).
The ratelimit length must be lower than the timeslice length.</p>
</dd>
</dl>
<p><strong>COMBINATION</strong></p>
<p>The following is the effect of combining the above options:</p>
<dl>
<dt><strong><a name="nothing_list_all_domain_params_and_sched_params_from_all_pools" class="item"><strong>&lt;nothing&gt;</strong>             : List all domain params and sched params from all pools</a></strong></dt>

<dt><strong><a name="d_domid_list_domain_params_for_domain_domid" class="item"><strong>-d [domid]</strong>            : List domain params for domain [domid]</a></strong></dt>

<dt><strong><a name="d_domid_params_set_domain_params_for_domain_domid" class="item"><strong>-d [domid] [params]</strong>   : Set domain params for domain [domid]</a></strong></dt>

<dt><strong><a name="p_pool_list_all_domains_and_sched_params_for_pool" class="item"><strong>-p [pool]</strong>             : list all domains and sched params for [pool]</a></strong></dt>

<dt><strong><a name="s_list_sched_params_for_poolid_0" class="item"><strong>-s</strong>                    : List sched params for poolid 0</a></strong></dt>

<dt><strong><a name="s_params_set_sched_params_for_poolid_0" class="item"><strong>-s [params]</strong>           : Set sched params for poolid 0</a></strong></dt>

<dt><strong><a name="p_pool_s_list_sched_params_for_pool" class="item"><strong>-p [pool] -s</strong>          : List sched params for [pool]</a></strong></dt>

<dt><strong><a name="p_pool_s_params_set_sched_params_for_pool" class="item"><strong>-p [pool] -s [params]</strong> : Set sched params for [pool]</a></strong></dt>

<dt><strong><a name="p_pool_d_illegal" class="item"><strong>-p [pool] -d</strong>...       : Illegal</a></strong></dt>

</dl>
</dd>
<dt><strong><a name="sched_credit2_options" class="item"><strong>sched-credit2</strong> [<em>OPTIONS</em>]</a></strong></dt>

<dd>
<p>Set or get credit2 scheduler parameters.  The credit2 scheduler is a
proportional fair share CPU scheduler built from the ground up to be
work conserving on SMP hosts.</p>
<p>Each domain (including Domain0) is assigned a weight.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="d_domain_domain_domain2" class="item"><strong>-d DOMAIN</strong>, <strong>--domain=DOMAIN</strong></a></strong></dt>

<dd>
<p>Specify domain for which scheduler parameters are to be modified or retrieved.
Mandatory for modifying scheduler parameters.</p>
</dd>
<dt><strong><a name="w_weight_weight_weight2" class="item"><strong>-w WEIGHT</strong>, <strong>--weight=WEIGHT</strong></a></strong></dt>

<dd>
<p>A domain with a weight of 512 will get twice as much CPU as a domain
with a weight of 256 on a contended host. Legal weights range from 1
to 65535 and the default is 256.</p>
</dd>
<dt><strong><a name="p_cpupool_cpupool_cpupool2" class="item"><strong>-p CPUPOOL</strong>, <strong>--cpupool=CPUPOOL</strong></a></strong></dt>

<dd>
<p>Restrict output to domains in the specified cpupool.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="sched_sedf_options" class="item"><strong>sched-sedf</strong> [<em>OPTIONS</em>]</a></strong></dt>

<dd>
<p>Set or get Simple EDF (Earliest Deadline First) scheduler parameters. This
scheduler provides weighted CPU sharing in an intuitive way and uses
realtime-algorithms to ensure time guarantees.  For more information see
docs/misc/sedf_scheduler_mini-HOWTO.txt in the Xen distribution.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="d_domain_domain_domain3" class="item"><strong>-d DOMAIN</strong>, <strong>--domain=DOMAIN</strong></a></strong></dt>

<dd>
<p>Specify domain for which scheduler parameters are to be modified or retrieved.
Mandatory for modifying scheduler parameters.</p>
</dd>
<dt><strong><a name="p_period_period_period" class="item"><strong>-p PERIOD</strong>, <strong>--period=PERIOD</strong></a></strong></dt>

<dd>
<p>The normal EDF scheduling usage in milliseconds.</p>
</dd>
<dt><strong><a name="s_slice_slice_slice" class="item"><strong>-s SLICE</strong>, <strong>--slice=SLICE</strong></a></strong></dt>

<dd>
<p>The normal EDF scheduling usage in milliseconds.</p>
</dd>
<dt><strong><a name="l_latency_latency_latency" class="item"><strong>-l LATENCY</strong>, <strong>--latency=LATENCY</strong></a></strong></dt>

<dd>
<p>Scaled period if domain is doing heavy I/O.</p>
</dd>
<dt><strong><a name="e_extra_extra_extra" class="item"><strong>-e EXTRA</strong>, <strong>--extra=EXTRA</strong></a></strong></dt>

<dd>
<p>Flag for allowing domain to run in extra time (0 or 1).</p>
</dd>
<dt><strong><a name="w_weight_weight_weight3" class="item"><strong>-w WEIGHT</strong>, <strong>--weight=WEIGHT</strong></a></strong></dt>

<dd>
<p>Another way of setting CPU slice.</p>
</dd>
<dt><strong><a name="c_cpupool_cpupool_cpupool" class="item"><strong>-c CPUPOOL</strong>, <strong>--cpupool=CPUPOOL</strong></a></strong></dt>

<dd>
<p>Restrict output to domains in the specified cpupool.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="sched_rtds_options" class="item"><strong>sched-rtds</strong> [<em>OPTIONS</em>]</a></strong></dt>

<dd>
<p>Set or get rtds (Real Time Deferrable Server) scheduler parameters.
This rt scheduler applies Preemptive Global Earliest Deadline First
real-time scheduling algorithm to schedule VCPUs in the system.
Each VCPU has a dedicated period and budget.
VCPUs in the same domain have the same period and budget.
While scheduled, a VCPU burns its budget.
A VCPU has its budget replenished at the beginning of each period;
Unused budget is discarded at the end of each period.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="d_domain_domain_domain4" class="item"><strong>-d DOMAIN</strong>, <strong>--domain=DOMAIN</strong></a></strong></dt>

<dd>
<p>Specify domain for which scheduler parameters are to be modified or retrieved.
Mandatory for modifying scheduler parameters.</p>
</dd>
<dt><strong><a name="p_period_period_period2" class="item"><strong>-p PERIOD</strong>, <strong>--period=PERIOD</strong></a></strong></dt>

<dd>
<p>Period of time, in microseconds, over which to replenish the budget.</p>
</dd>
<dt><strong><a name="b_budget_budget_budget" class="item"><strong>-b BUDGET</strong>, <strong>--budget=BUDGET</strong></a></strong></dt>

<dd>
<p>Amount of time, in microseconds, that the VCPU will be allowed
to run every period.</p>
</dd>
<dt><strong><a name="c_cpupool_cpupool_cpupool2" class="item"><strong>-c CPUPOOL</strong>, <strong>--cpupool=CPUPOOL</strong></a></strong></dt>

<dd>
<p>Restrict output to domains in the specified cpupool.</p>
</dd>
</dl>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="cpupools_commands">CPUPOOLS COMMANDS</a></h1>
<p>Xen can group the physical cpus of a server in cpu-pools. Each physical CPU is
assigned at most to one cpu-pool. Domains are each restricted to a single
cpu-pool. Scheduling does not cross cpu-pool boundaries, so each cpu-pool has
an own scheduler.
Physical cpus and domains can be moved from one cpu-pool to another only by an
explicit command.
Cpu-pools can be specified either by name or by id.</p>
<dl>
<dt><strong><a name="cpupool_create_options_configfile_variable_value" class="item"><strong>cpupool-create</strong> [<em>OPTIONS</em>] [<em>ConfigFile</em>] [<em>Variable=Value</em> ...]</a></strong></dt>

<dd>
<p>Create a cpu pool based an config from a <em>ConfigFile</em> or command-line
parameters.  Variable settings from the <em>ConfigFile</em> may be altered
by specifying new or additional assignments on the command line.</p>
<p>See the <em>xlcpupool.cfg(5)</em> manpage for more information.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="f_file_defconfig_file3" class="item"><strong>-f=FILE</strong>, <strong>--defconfig=FILE</strong></a></strong></dt>

<dd>
<p>Use the given configuration file.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="cpupool_list_c_cpus_cpu_pool" class="item"><strong>cpupool-list</strong> [<em>-c|--cpus</em>] [<em>cpu-pool</em>]</a></strong></dt>

<dd>
<p>List CPU pools on the host.
If <em>-c</em> is specified, <strong>xl</strong> prints a list of CPUs used by <em>cpu-pool</em>.</p>
</dd>
<dt><strong><a name="cpupool_destroy_cpu_pool" class="item"><strong>cpupool-destroy</strong> <em>cpu-pool</em></a></strong></dt>

<dd>
<p>Deactivates a cpu pool.
This is possible only if no domain is active in the cpu-pool.</p>
</dd>
<dt><strong><a name="cpupool_rename_cpu_pool_newname" class="item"><strong>cpupool-rename</strong> <em>cpu-pool</em> &lt;newname&gt;</a></strong></dt>

<dd>
<p>Renames a cpu-pool to <em>newname</em>.</p>
</dd>
<dt><strong><a name="cpupool_cpu_add_cpu_pool_cpu_nr_node_node_nr" class="item"><strong>cpupool-cpu-add</strong> <em>cpu-pool</em> <em>cpu-nr|node:node-nr</em></a></strong></dt>

<dd>
<p>Adds a cpu or all cpus of a numa node to a cpu-pool.</p>
</dd>
<dt><strong><a name="cpupool_cpu_remove_cpu_nr_node_node_nr" class="item"><strong>cpupool-cpu-remove</strong> <em>cpu-nr|node:node-nr</em></a></strong></dt>

<dd>
<p>Removes a cpu or all cpus of a numa node from a cpu-pool.</p>
</dd>
<dt><strong><a name="cpupool_migrate_domain_cpu_pool" class="item"><strong>cpupool-migrate</strong> <em>domain</em> <em>cpu-pool</em></a></strong></dt>

<dd>
<p>Moves a domain specified by domain-id or domain-name into a cpu-pool.</p>
</dd>
<dt><strong><a name="cpupool_numa_split" class="item"><strong>cpupool-numa-split</strong></a></strong></dt>

<dd>
<p>Splits up the machine into one cpu-pool per numa node.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="virtual_device_commands">VIRTUAL DEVICE COMMANDS</a></h1>
<p>Most virtual devices can be added and removed while guests are
running, assuming that the necessary support exists in the guest.  The
effect to the guest OS is much the same as any hotplug event.</p>
<p>
</p>
<h2><a name="block_devices">BLOCK DEVICES</a></h2>
<dl>
<dt><strong><a name="component" class="item"><strong>block-attach</strong> <em>domain-id</em> <em>disc-spec-component(s)</em> ...</a></strong></dt>

<dd>
<p>Create a new virtual block device.  This will trigger a hotplug event
for the guest.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><em>domain-id</em></strong></dt>

<dd>
<p>The domain id of the guest domain that the device will be attached to.</p>
</dd>
<dt><strong><a name="disc_spec_component" class="item"><em>disc-spec-component</em></a></strong></dt>

<dd>
<p>A disc specification in the same format used for the <strong>disk</strong> variable in
the domain config file. See
<a href="http://xenbits.xen.org/docs/unstable/misc/xl-disk-configuration.txt">http://xenbits.xen.org/docs/unstable/misc/xl-disk-configuration.txt</a>.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="block_detach_domain_id_devid_force" class="item"><strong>block-detach</strong> <em>domain-id</em> <em>devid</em> [<strong>--force</strong>]</a></strong></dt>

<dd>
<p>Detach a domain's virtual block device. <em>devid</em> may be the symbolic
name or the numeric device id given to the device by domain 0.  You
will need to run <strong>xl block-list</strong> to determine that number.</p>
<p>Detaching the device requires the cooperation of the domain.  If the
domain fails to release the device (perhaps because the domain is hung
or is still using the device), the detach will fail.  The <strong>--force</strong>
parameter will forcefully detach the device, but may cause IO errors
in the domain.</p>
</dd>
<dt><strong><a name="block_list_domain_id" class="item"><strong>block-list</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>List virtual block devices for a domain.</p>
</dd>
<dt><strong><a name="cd_insert_domain_id_virtualdevice_target" class="item"><strong>cd-insert</strong> <em>domain-id</em> <em>VirtualDevice</em> <em>target</em></a></strong></dt>

<dd>
<p>Insert a cdrom into a guest domain's existing virtial cd drive. The
virtual drive must already exist but can be current empty.</p>
<p>Only works with HVM domains.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="virtualdevice" class="item"><em>VirtualDevice</em></a></strong></dt>

<dd>
<p>How the device should be presented to the guest domain; for example &quot;hdc&quot;.</p>
</dd>
<dt><strong><a name="target" class="item"><em>target</em></a></strong></dt>

<dd>
<p>the target path in the backend domain (usually domain 0) to be
exported; Can be a block device or a file etc. See <strong>target</strong> in
<em class="file">docs/misc/xl-disk-configuration.txt</em>.</p>
</dd>
</dl>
</dd>
<dt><strong><a name="cd_eject_domain_id_virtualdevice" class="item"><strong>cd-eject</strong> <em>domain-id</em> <em>VirtualDevice</em></a></strong></dt>

<dd>
<p>Eject a cdrom from a guest's virtual cd drive. Only works with HVM domains.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="virtualdevice2" class="item"><em>VirtualDevice</em></a></strong></dt>

<dd>
<p>How the device should be presented to the guest domain; for example &quot;hdc&quot;.</p>
</dd>
</dl>
</dd>
</dl>
<p>
</p>
<h2><a name="network_devices">NETWORK DEVICES</a></h2>
<dl>
<dt><strong><a name="network_attach_domain_id_network_device" class="item"><strong>network-attach</strong> <em>domain-id</em> <em>network-device</em></a></strong></dt>

<dd>
<p>Creates a new network device in the domain specified by <em>domain-id</em>.
<em>network-device</em> describes the device to attach, using the same format as the
<strong>vif</strong> string in the domain config file. See <em>xl.cfg</em> and
<a href="http://xenbits.xen.org/docs/unstable/misc/xl-network-configuration.html">http://xenbits.xen.org/docs/unstable/misc/xl-network-configuration.html</a>
for more informations.</p>
</dd>
<dt><strong><a name="network_detach_domain_id_devid_mac" class="item"><strong>network-detach</strong> <em>domain-id</em> <em>devid|mac</em></a></strong></dt>

<dd>
<p>Removes the network device from the domain specified by <em>domain-id</em>.
<em>devid</em> is the virtual interface device number within the domain
(i.e. the 3 in vif22.3). Alternatively the <em>mac</em> address can be used to
select the virtual interface to detach.</p>
</dd>
<dt><strong><a name="network_list_domain_id" class="item"><strong>network-list</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>List virtual network interfaces for a domain.</p>
</dd>
</dl>
<p>
</p>
<h2><a name="channel_devices">CHANNEL DEVICES</a></h2>
<dl>
<dt><strong><a name="channel_list_domain_id" class="item"><strong>channel-list</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>List virtual channel interfaces for a domain.</p>
</dd>
</dl>
<p>
</p>
<h2><a name="vtpm_devices">VTPM DEVICES</a></h2>
<dl>
<dt><strong><a name="vtpm_attach_domain_id_vtpm_device" class="item"><strong>vtpm-attach</strong> <em>domain-id</em> <em>vtpm-device</em></a></strong></dt>

<dd>
<p>Creates a new vtpm device in the domain specified by <em>domain-id</em>.
<em>vtpm-device</em> describes the device to attach, using the same format as the
<strong>vtpm</strong> string in the domain config file. See <em>xl.cfg</em> for
more information.</p>
</dd>
<dt><strong><a name="vtpm_detach_domain_id_devid_uuid" class="item"><strong>vtpm-detach</strong> <em>domain-id</em> <em>devid|uuid</em></a></strong></dt>

<dd>
<p>Removes the vtpm device from the domain specified by <em>domain-id</em>.
<em>devid</em> is the numeric device id given to the virtual trusted
platform module device. You will need to run <strong>xl vtpm-list</strong> to determine that number.
Alternatively the <em>uuid</em> of the vtpm can be used to
select the virtual device to detach.</p>
</dd>
<dt><strong><a name="vtpm_list_domain_id" class="item"><strong>vtpm-list</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>List virtual trusted platform modules for a domain.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="pci_pass_through">PCI PASS-THROUGH</a></h1>
<dl>
<dt><strong><a name="pci_assignable_list" class="item"><strong>pci-assignable-list</strong></a></strong></dt>

<dd>
<p>List all the assignable PCI devices.
These are devices in the system which are configured to be
available for passthrough and are bound to a suitable PCI
backend driver in domain 0 rather than a real driver.</p>
</dd>
<dt><strong><a name="pci_assignable_add_bdf" class="item"><strong>pci-assignable-add</strong> <em>BDF</em></a></strong></dt>

<dd>
<p>Make the device at PCI Bus/Device/Function BDF assignable to guests.
This will bind the device to the pciback driver.  If it is already
bound to a driver, it will first be unbound, and the original driver
stored so that it can be re-bound to the same driver later if desired.
If the device is already bound, it will return success.</p>
<p>CAUTION: This will make the device unusable by Domain 0 until it is
returned with pci-assignable-remove.  Care should therefore be taken
not to do this on a device critical to domain 0's operation, such as
storage controllers, network interfaces, or GPUs that are currently
being used.</p>
</dd>
<dt><strong><a name="pci_assignable_remove_r_bdf" class="item"><strong>pci-assignable-remove</strong> [<em>-r</em>] <em>BDF</em></a></strong></dt>

<dd>
<p>Make the device at PCI Bus/Device/Function BDF assignable to guests.  This
will at least unbind the device from pciback.  If the -r option is specified,
it will also attempt to re-bind the device to its original driver, making it
usable by Domain 0 again.  If the device is not bound to pciback, it will
return success.</p>
</dd>
<dt><strong><a name="pci_attach_domain_id_bdf" class="item"><strong>pci-attach</strong> <em>domain-id</em> <em>BDF</em></a></strong></dt>

<dd>
<p>Hot-plug a new pass-through pci device to the specified domain.
<strong>BDF</strong> is the PCI Bus/Device/Function of the physical device to pass-through.</p>
</dd>
<dt><strong><a name="pci_detach_f_domain_id_bdf" class="item"><strong>pci-detach</strong> [<em>-f</em>] <em>domain-id</em> <em>BDF</em></a></strong></dt>

<dd>
<p>Hot-unplug a previously assigned pci device from a domain. <strong>BDF</strong> is the PCI
Bus/Device/Function of the physical device to be removed from the guest domain.</p>
<p>If <strong>-f</strong> is specified, <strong>xl</strong> is going to forcefully remove the device even
without guest's collaboration.</p>
</dd>
<dt><strong><a name="pci_list_domain_id" class="item"><strong>pci-list</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>List pass-through pci devices for a domain.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="tmem">TMEM</a></h1>
<dl>
<dt><strong><a name="tmem_list_i_l_domain_id" class="item"><strong>tmem-list</strong> I[&lt;-l&gt;] <em>domain-id</em></a></strong></dt>

<dd>
<p>List tmem pools. If <em>-l</em> is specified, also list tmem stats.</p>
</dd>
<dt><strong><a name="tmem_freeze_domain_id" class="item"><strong>tmem-freeze</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>Freeze tmem pools.</p>
</dd>
<dt><strong><a name="tmem_thaw_domain_id" class="item"><strong>tmem-thaw</strong> <em>domain-id</em></a></strong></dt>

<dd>
<p>Thaw tmem pools.</p>
</dd>
<dt><strong><a name="tmem_set_domain_id_options" class="item"><strong>tmem-set</strong> <em>domain-id</em> [<em>OPTIONS</em>]</a></strong></dt>

<dd>
<p>Change tmem settings.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="w_weight" class="item"><strong>-w</strong> <em>WEIGHT</em></a></strong></dt>

<dd>
<p>Weight (int)</p>
</dd>
<dt><strong><a name="c_cap" class="item"><strong>-c</strong> <em>CAP</em></a></strong></dt>

<dd>
<p>Cap (int)</p>
</dd>
<dt><strong><a name="p_compress" class="item"><strong>-p</strong> <em>COMPRESS</em></a></strong></dt>

<dd>
<p>Compress (int)</p>
</dd>
</dl>
</dd>
<dt><strong><a name="tmem_shared_auth_domain_id_options" class="item"><strong>tmem-shared-auth</strong> <em>domain-id</em> [<em>OPTIONS</em>]</a></strong></dt>

<dd>
<p>De/authenticate shared tmem pool.</p>
<p><strong>OPTIONS</strong></p>
<dl>
<dt><strong><a name="u_uuid" class="item"><strong>-u</strong> <em>UUID</em></a></strong></dt>

<dd>
<p>Specify uuid (abcdef01-2345-6789-1234-567890abcdef)</p>
</dd>
<dt><strong><a name="a_auth" class="item"><strong>-a</strong> <em>AUTH</em></a></strong></dt>

<dd>
<p>0=auth,1=deauth</p>
</dd>
</dl>
</dd>
<dt><strong><a name="tmem_freeable" class="item"><strong>tmem-freeable</strong></a></strong></dt>

<dd>
<p>Get information about how much freeable memory (MB) is in-use by tmem.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="flask">FLASK</a></h1>
<p><strong>FLASK</strong> is a security framework that defines a mandatory access control policy
providing fine-grained controls over Xen domains, allowing the policy writer
to define what interactions between domains, devices, and the hypervisor are
permitted. Some example of what you can do using XSM/FLASK:
 - Prevent two domains from communicating via event channels or grants
 - Control which domains can use device passthrough (and which devices)
 - Restrict or audit operations performed by privileged domains
 - Prevent a privileged domain from arbitrarily mapping pages from other
   domains.</p>
<p>You can find more details on how to use FLASK and an example security
policy here: <a href="http://xenbits.xen.org/docs/unstable/misc/xsm-flask.txt">http://xenbits.xen.org/docs/unstable/misc/xsm-flask.txt</a></p>
<dl>
<dt><strong><a name="getenforce" class="item"><strong>getenforce</strong></a></strong></dt>

<dd>
<p>Determine if the FLASK security module is loaded and enforcing its policy.</p>
</dd>
<dt><strong><a name="setenforce_1_0_enforcing_permissive" class="item"><strong>setenforce</strong> <em>1|0|Enforcing|Permissive</em></a></strong></dt>

<dd>
<p>Enable or disable enforcing of the FLASK access controls. The default is
permissive and can be changed using the flask_enforcing option on the
hypervisor's command line.</p>
</dd>
<dt><strong><a name="loadpolicy_policy_file" class="item"><strong>loadpolicy</strong> <em>policy-file</em></a></strong></dt>

<dd>
<p>Load FLASK policy from the given policy file. The initial policy is provided to
the hypervisor as a multiboot module; this command allows runtime updates to the
policy. Loading new security policy will reset runtime changes to device labels.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="cache_monitoring_technology">CACHE MONITORING TECHNOLOGY</a></h1>
<p>Intel Haswell and later server platforms offer monitoring capability in each
logical processor to measure specific platform shared resource metric, for
example, L3 cache occupancy. In Xen implementation, the monitoring granularity
is domain level. To monitor a specific domain, just attach the domain id with
the monitoring service. When the domain doesn't need to be monitored any more,
detach the domain id from the monitoring service.</p>
<dl>
<dt><strong><a name="psr_cmt_attach_domain_id" class="item"><strong>psr-cmt-attach</strong> [<em>domain-id</em>]</a></strong></dt>

<dd>
<p>attach: Attach the platform shared resource monitoring service to a domain.</p>
</dd>
<dt><strong><a name="psr_cmt_detach_domain_id" class="item"><strong>psr-cmt-detach</strong> [<em>domain-id</em>]</a></strong></dt>

<dd>
<p>detach: Detach the platform shared resource monitoring service from a domain.</p>
</dd>
<dt><strong><a name="psr_cmt_show_psr_monitor_type_domain_id" class="item"><strong>psr-cmt-show</strong> [<em>psr-monitor-type</em>] [<em>domain-id</em>]</a></strong></dt>

<dd>
<p>Show monitoring data for a certain domain or all domains. Current supported
monitor types are:
 - &quot;cache-occupancy&quot;: showing the L3 cache occupancy.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="to_be_documented">TO BE DOCUMENTED</a></h1>
<p>We need better documentation for:</p>
<dl>
<dt><strong><a name="tmem" class="item"><strong>tmem</strong></a></strong></dt>

<dd>
<p>Transcendent Memory.</p>
</dd>
</dl>
<p>
</p>
<hr />
<h1><a name="see_also">SEE ALSO</a></h1>
<p>The following man pages:</p>
<p><em>xl.cfg</em>(5), <em>xlcpupool.cfg</em>(5), <strong>xentop</strong>(1)</p>
<p>And the following documents on the xen.org website:</p>
<p><a href="http://xenbits.xen.org/docs/unstable/misc/xl-network-configuration.html">http://xenbits.xen.org/docs/unstable/misc/xl-network-configuration.html</a>
<a href="http://xenbits.xen.org/docs/unstable/misc/xl-disk-configuration.txt">http://xenbits.xen.org/docs/unstable/misc/xl-disk-configuration.txt</a>
<a href="http://xenbits.xen.org/docs/unstable/misc/xsm-flask.txt">http://xenbits.xen.org/docs/unstable/misc/xsm-flask.txt</a></p>
<p>For systems that don't automatically bring CPU online:</p>
<p><a href="http://wiki.xen.org/wiki/Paravirt_Linux_CPU_Hotplug">http://wiki.xen.org/wiki/Paravirt_Linux_CPU_Hotplug</a></p>
<p>
</p>
<hr />
<h1><a name="bugs">BUGS</a></h1>
<p>Send bugs to <a href="mailto:xen-devel@lists.xen.org,">xen-devel@lists.xen.org,</a> see
<a href="http://wiki.xen.org/xenwiki/ReportingBugs">http://wiki.xen.org/xenwiki/ReportingBugs</a> on how to send bug reports.</p>

</body>

</html>
